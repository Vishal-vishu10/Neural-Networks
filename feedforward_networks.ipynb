{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Ak4If2D-6M",
        "outputId": "acc88f7b-410b-423c-e9b3-e2d3605f56fd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load nn.py\n",
        "\"\"\"Untitled6.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ppF-j7AB0BJnVeAdD3ZlnejCuQ7G0wMX\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "def create_auto_mpg_deep_and_wide_networks(n_inputs: int, n_outputs: int) -> Tuple[keras.Model, keras.Model]:\n",
        "    \"\"\"Creates one deep neural network and one wide neural network.\"\"\"\n",
        "\n",
        "    # Deep Network\n",
        "    deep_model = keras.Sequential([\n",
        "        layers.Input(shape=(n_inputs,)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(n_outputs)\n",
        "    ])\n",
        "\n",
        "    # Wide Network (shallower but with wider layers to match parameters)\n",
        "    wide_model = keras.Sequential([\n",
        "        layers.Input(shape=(n_inputs,)),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(n_outputs)\n",
        "    ])\n",
        "\n",
        "    return deep_model, wide_model\n",
        "\n",
        "def create_activity_dropout_and_nodropout_networks(n_inputs: int, n_outputs: int) -> Tuple[keras.Model, keras.Model]:\n",
        "    \"\"\"Creates one neural network with dropout and one without dropout.\"\"\"\n",
        "\n",
        "    # Dropout Network\n",
        "    dropout_model = keras.Sequential([\n",
        "        layers.Input(shape=(n_inputs,)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(n_outputs, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # No Dropout Network\n",
        "    nodropout_model = keras.Sequential([\n",
        "        layers.Input(shape=(n_inputs,)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return dropout_model, nodropout_model\n",
        "\n",
        "def create_income_earlystopping_and_noearlystopping_networks(n_inputs: int, n_outputs: int) -> Tuple[keras.Model, Dict, keras.Model, Dict]:\n",
        "    \"\"\"Creates one neural network that uses early stopping and one that does not.\"\"\"\n",
        "\n",
        "    # Identical Model Architecture\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(n_inputs,)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    return model, {'callbacks': [early_stopping]}, model, {}"
      ],
      "metadata": {
        "id": "8KraeMgbELz0"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nn.py', 'r') as file:\n",
        "    code = file.read()"
      ],
      "metadata": {
        "id": "GMDGVG7wEL81"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tUHxeRREMF7",
        "outputId": "e0059794-ac40-4586-bb6a-322017cd8c59"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"nn.ipynb\n",
            "\n",
            "Automatically generated by Colab.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1ppF-j7AB0BJnVeAdD3ZlnejCuQ7G0wMX\n",
            "\"\"\"\n",
            "\n",
            "import tensorflow as tf\n",
            "from tensorflow import keras\n",
            "from tensorflow.keras import layers\n",
            "from typing import Tuple, Dict\n",
            "\n",
            "\n",
            "def create_income_earlystopping_and_noearlystopping_networks(n_inputs: int, n_outputs: int):\n",
            "    \"\"\"Creates two neural networks for income prediction: one with early stopping, one without.\"\"\"\n",
            "\n",
            "    # Model architecture\n",
            "    def build_model():\n",
            "        model = keras.Sequential([\n",
            "            layers.Input(shape=(n_inputs,)),\n",
            "            layers.Dense(256, activation='relu'),  # Increased neurons\n",
            "            layers.Dense(128, activation='relu'),\n",
            "            layers.Dense(64, activation='relu'),\n",
            "            layers.Dense(n_outputs, activation='sigmoid')  # Binary classification\n",
            "        ])\n",
            "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
            "        return model\n",
            "\n",
            "    early_model = build_model()\n",
            "    late_model = build_model()\n",
            "\n",
            "    # Early stopping callback with increased patience\n",
            "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
            "\n",
            "    return early_model, {'callbacks': [early_stopping], 'epochs': 50}, late_model, {'epochs': 50}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load test_nn.py\n",
        "\"\"\"test_nn.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ihIJwAhg0nrM61nSlSeFGa64u4mtgl86\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
        "\n",
        "# Create the 'data/' directory if it doesn't exist\n",
        "data_dir = 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Download data using requests\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Data downloaded successfully.\")\n",
        "\n",
        "    data_content = response.text\n",
        "    df = pd.read_csv(StringIO(data_content), header=None, sep=\"\\s+\", na_values=\"?\", names=[\n",
        "        \"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\",\n",
        "        \"acceleration\", \"model year\", \"origin\", \"carname\"])\n",
        "\n",
        "    df = df.dropna().drop(\"carname\", axis=1)\n",
        "    input_df = df.drop(\"mpg\", axis=1)\n",
        "    output_df = df[[\"mpg\"]]\n",
        "\n",
        "    mask = np.random.rand(len(df)) < 0.8\n",
        "    train_input = input_df[mask].values\n",
        "    train_output = output_df[mask].values\n",
        "    test_input = input_df[~mask].values\n",
        "    test_output = output_df[~mask].values\n",
        "\n",
        "    with h5py.File(os.path.join(data_dir, 'auto-mpg.hdf5'), 'w') as f:\n",
        "        train = f.create_group(\"train\")\n",
        "        train.create_dataset(\"input\", compression=\"gzip\", data=train_input)\n",
        "        train.create_dataset(\"output\", compression=\"gzip\", data=train_output)\n",
        "\n",
        "        test = f.create_group(\"test\")\n",
        "        test.create_dataset(\"input\", compression=\"gzip\", data=test_input)\n",
        "        test.create_dataset(\"output\", compression=\"gzip\", data=test_output)\n",
        "\n",
        "    print(\"Data saved to 'auto-mpg.hdf5'.\")\n",
        "else:\n",
        "    print(f\"Failed to download data. HTTP Status Code: {response.status_code}\")\n",
        "\n",
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import h5py\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Specify the URL for the UCI HAR Dataset.zip file\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\"\n",
        "\n",
        "\n",
        "# Download and extract the dataset\n",
        "with zipfile.ZipFile(io.BytesIO(urllib.request.urlopen(url).read()), 'r') as zip:\n",
        "    train_input = np.loadtxt(zip.extract(\"UCI HAR Dataset/train/X_train.txt\"))\n",
        "    train_output = to_categorical(np.loadtxt(zip.extract(\"UCI HAR Dataset/train/y_train.txt\")))\n",
        "    test_input = np.loadtxt(zip.extract(\"UCI HAR Dataset/test/X_test.txt\"))\n",
        "    test_output = to_categorical(np.loadtxt(zip.extract(\"UCI HAR Dataset/test/y_test.txt\")))\n",
        "\n",
        "# Create an HDF5 file to store the data\n",
        "hdf5_path = 'data/uci-har.hdf5'\n",
        "with h5py.File(hdf5_path, 'w') as f:\n",
        "    train = f.create_group(\"train\")\n",
        "    train.create_dataset(\"input\", compression=\"gzip\", data=train_input, dtype=np.dtype(\"f2\"))\n",
        "    train.create_dataset(\"output\", compression=\"gzip\", data=train_output, dtype=np.dtype(\"i1\"))\n",
        "    test = f.create_group(\"test\")\n",
        "    test.create_dataset(\"input\", compression=\"gzip\", data=test_input, dtype=np.dtype(\"f2\"))\n",
        "    test.create_dataset(\"output\", compression=\"gzip\", data=test_output, dtype=np.dtype(\"i1\"))\n",
        "\n",
        "# Print a message indicating that the data has been downloaded and saved\n",
        "print(\"Data downloaded successfully.\")\n",
        "print(\"Data saved to:\", hdf5_path)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "column_names = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\",\n",
        "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
        "    \"hours_per_week\", \"native_country\", \"income\"\n",
        "]\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(url, header=None, sep=r\"\\s*,\\s*\", na_values=\"?\", engine=\"python\", names=column_names)\n",
        "    df = df.dropna()\n",
        "    df = pd.get_dummies(df)\n",
        "    df = df.drop(\"income_<=50K\", axis=1)\n",
        "    input_df = df.drop(\"income_>50K\", axis=1)\n",
        "    output_df = df[[\"income_>50K\"]]\n",
        "\n",
        "    mask = np.random.rand(len(df)) < 0.8\n",
        "    train_input = input_df[mask].values.astype(np.float32)  # Convert to float32\n",
        "    train_output = output_df[mask].values.astype(np.int32)   # Convert to int32\n",
        "    test_input = input_df[~mask].values.astype(np.float32)   # Convert to float32\n",
        "    test_output = output_df[~mask].values.astype(np.int32)    # Convert to int32\n",
        "\n",
        "    output_dir = 'data'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    file_path = os.path.join(output_dir, 'income.hdf5')\n",
        "\n",
        "    with h5py.File(file_path, 'w') as f:\n",
        "        train = f.create_group(\"train\")\n",
        "        train.create_dataset(\"input\", compression=\"gzip\", data=train_input, dtype='f')\n",
        "        train.create_dataset(\"output\", compression=\"gzip\", data=train_output, dtype='i')\n",
        "\n",
        "        test = f.create_group(\"test\")\n",
        "        test.create_dataset(\"input\", compression=\"gzip\", data=test_input, dtype='f')\n",
        "        test.create_dataset(\"output\", compression=\"gzip\", data=test_output, dtype='i')\n",
        "\n",
        "    print(\"Data downloaded successfully.\")\n",
        "    print(f\"Training data shape: {train_input.shape}, {train_output.shape}\")\n",
        "    print(f\"Testing data shape: {test_input.shape}, {test_output.shape}\")\n",
        "    print(f\"Data saved to '{file_path}'.\")\n",
        "\n",
        "except HTTPError as e:\n",
        "    print(f\"An error occurred while trying to download the data: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import tensorflow\n",
        "\n",
        "import nn\n",
        "\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def set_seeds():\n",
        "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "    tensorflow.random.set_seed(42)\n",
        "    tensorflow.config.threading.set_intra_op_parallelism_threads(1)\n",
        "    tensorflow.config.threading.set_inter_op_parallelism_threads(1)\n",
        "\n",
        "\n",
        "def test_deep_vs_wide(capsys):\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/auto-mpg.hdf5\")\n",
        "\n",
        "    deep, wide = nn.create_auto_mpg_deep_and_wide_networks(\n",
        "        train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # check that the deep neural network is indeed deeper\n",
        "    assert len(deep.layers) > len(wide.layers)\n",
        "\n",
        "    # check that the 2 networks have (nearly) the same number of parameters\n",
        "    params1 = deep.count_params()\n",
        "    params2 = wide.count_params()\n",
        "    assert abs(params1 - params2) / (params1 + params2) < 0.05\n",
        "\n",
        "    # check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(deep, wide)\n",
        "\n",
        "    # check that the 2 networks have the same activation functions\n",
        "    assert set(hidden_activations(deep)) == set(hidden_activations(wide))\n",
        "\n",
        "    # check that output type and loss are appropriate for regression\n",
        "    assert all(\"mean\" in loss_name(model) for model in [deep, wide])\n",
        "    assert loss_name(deep) == loss_name(wide)\n",
        "    assert output_activation(deep) == output_activation(wide) == \\\n",
        "        tensorflow.keras.activations.linear\n",
        "\n",
        "    # train both networks\n",
        "    deep.fit(train_in, train_out, verbose=0, epochs=100)\n",
        "    wide.fit(train_in, train_out, verbose=0, epochs=100)\n",
        "\n",
        "    # check that error level is acceptable\n",
        "    mean_predict = np.full(shape=test_out.shape, fill_value=np.mean(train_out))\n",
        "    [baseline_rmse] = root_mean_squared_error(mean_predict, test_out)\n",
        "    [deep_rmse] = root_mean_squared_error(deep.predict(test_in), test_out)\n",
        "    [wide_rmse] = root_mean_squared_error(wide.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        rmse_format = \"{1:.1f} RMSE for {0} on Auto MPG\".format\n",
        "        print()\n",
        "        print(rmse_format(\"baseline\", baseline_rmse))\n",
        "        print(rmse_format(\"deep\", deep_rmse))\n",
        "        print(rmse_format(\"wide\", wide_rmse))\n",
        "\n",
        "    assert deep_rmse < baseline_rmse\n",
        "    assert wide_rmse < baseline_rmse\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_dropout(capsys):\n",
        "\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/uci-har.hdf5\")\n",
        "\n",
        "    # keep only every 10th training example\n",
        "    train_out = train_out[::10, :]\n",
        "    train_in = train_in[::10, :]\n",
        "\n",
        "    drop, no_drop = nn.create_activity_dropout_and_nodropout_networks(\n",
        "        train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # check that the dropout network has Dropout and the other doesn't\n",
        "    assert any(isinstance(layer, tensorflow.keras.layers.Dropout)\n",
        "               for layer in drop.layers)\n",
        "    assert all(not isinstance(layer, tensorflow.keras.layers.Dropout)\n",
        "               for layer in no_drop.layers)\n",
        "\n",
        "    # check that the 2 networks have the same number of parameters\n",
        "    assert drop.count_params() == no_drop.count_params()\n",
        "\n",
        "    # check that the two networks are identical other than dropout\n",
        "    dropped_dropout = [l for l in drop.layers\n",
        "                       if not isinstance(l, tensorflow.keras.layers.Dropout)]\n",
        "    assert_layers_equal(dropped_dropout, no_drop.layers)\n",
        "\n",
        "    # check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(drop, no_drop)\n",
        "\n",
        "    # check that output type and loss are appropriate for multi-class\n",
        "    assert all(\"categorical\" in loss_name(model)\n",
        "               for model in [drop, no_drop])\n",
        "    assert loss_name(drop) == loss_name(no_drop)\n",
        "    assert output_activation(drop) == output_activation(no_drop) == \\\n",
        "        tensorflow.keras.activations.softmax\n",
        "\n",
        "    # train both networks\n",
        "    drop.fit(train_in, train_out, verbose=0, epochs=10)\n",
        "    no_drop.fit(train_in, train_out, verbose=0, epochs=10)\n",
        "\n",
        "    # check that accuracy level is acceptable\n",
        "    baseline_prediction = np.zeros_like(test_out)\n",
        "    baseline_prediction[:, np.argmax(np.sum(train_out, axis=0), axis=0)] = 1\n",
        "    baseline_accuracy = multi_class_accuracy(baseline_prediction, test_out)\n",
        "    dropout_accuracy = multi_class_accuracy(drop.predict(test_in), test_out)\n",
        "    no_dropout_accuracy = multi_class_accuracy(\n",
        "        no_drop.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        accuracy_format = \"{1:.1%} accuracy for {0} on UCI-HAR\".format\n",
        "        print()\n",
        "        print(accuracy_format(\"baseline\", baseline_accuracy))\n",
        "        print(accuracy_format(\"dropout\", dropout_accuracy))\n",
        "        print(accuracy_format(\"no dropout\", no_dropout_accuracy))\n",
        "    assert dropout_accuracy >= 0.75\n",
        "    assert no_dropout_accuracy >= 0.75\n",
        "\n",
        "\n",
        "def test_early_stopping(capsys):\n",
        "    train_in, train_out, test_in, test_out = load_hdf5(\"data/income.hdf5\")\n",
        "\n",
        "    # Keep only every 5th training example instead of 10 for more data\n",
        "    train_out = train_out[::5, :]\n",
        "    train_in = train_in[::5, :]\n",
        "\n",
        "    early, early_fit_kwargs, late, late_fit_kwargs = \\\n",
        "        nn.create_income_earlystopping_and_noearlystopping_networks(\n",
        "            train_in.shape[-1], train_out.shape[-1])\n",
        "\n",
        "    # Check that the two networks have the same number of parameters\n",
        "    assert early.count_params() == late.count_params()\n",
        "\n",
        "    # Check that the two networks have identical layers\n",
        "    assert_layers_equal(early.layers, late.layers)\n",
        "\n",
        "    # Check that the 2 networks have the same compile parameters\n",
        "    assert_compile_parameters_equal(early, late)\n",
        "\n",
        "    # Check that output type and loss are appropriate for binary classification\n",
        "    assert all(any(x in loss_name(model) for x in {\"crossentropy\", \"hinge\"})\n",
        "               and \"categorical\" not in loss_name(model)\n",
        "               for model in [early, late])\n",
        "    assert loss_name(early) == loss_name(late)\n",
        "    assert output_activation(early) == output_activation(late) == \\\n",
        "        tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # Train both networks with increased epochs and a validation set for early stopping\n",
        "    early_fit_kwargs.update(verbose=0, epochs=100, validation_data=(test_in, test_out))\n",
        "    early_hist = early.fit(train_in, train_out, **early_fit_kwargs)\n",
        "\n",
        "    late_fit_kwargs.update(verbose=0, epochs=100)  # More epochs for late\n",
        "    late_hist = late.fit(train_in, train_out, **late_fit_kwargs)\n",
        "\n",
        "    # Check that accuracy levels are acceptable\n",
        "    all1_accuracy = np.sum(test_out == 1) / test_out.size\n",
        "    early_accuracy = binary_accuracy(early.predict(test_in), test_out)\n",
        "    late_accuracy = binary_accuracy(late.predict(test_in), test_out)\n",
        "\n",
        "    with capsys.disabled():\n",
        "        accuracy_format = \"{1:.1%} accuracy for {0} on census income\".format\n",
        "        print()\n",
        "        print(accuracy_format(\"baseline\", all1_accuracy))\n",
        "        print(accuracy_format(\"early\", early_accuracy))\n",
        "        print(accuracy_format(\"late\", late_accuracy))\n",
        "\n",
        "    assert early_accuracy > 0.75\n",
        "    assert late_accuracy > 0.75\n",
        "    assert early_accuracy > all1_accuracy\n",
        "    assert late_accuracy > all1_accuracy\n",
        "\n",
        "    # Check that the first network stopped early (fewer epochs)\n",
        "    assert len(early_hist.history[\"loss\"]) < len(late_hist.history[\"loss\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_hdf5(path):\n",
        "    with h5py.File(path, 'r') as f:\n",
        "        train = f[\"train\"]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        train_in = np.array(train[\"input\"])\n",
        "        test = f[\"test\"]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "        test_in = np.array(test[\"input\"])\n",
        "    return train_in, train_out, test_in, test_out\n",
        "\n",
        "\n",
        "def assert_layers_equal(layers1: List[tensorflow.keras.layers.Layer],\n",
        "                        layers2: List[tensorflow.keras.layers.Layer]):\n",
        "    def layer_info(layer):\n",
        "        return (layer.__class__,\n",
        "                getattr(layer, \"units\", None),\n",
        "                getattr(layer, \"activation\", None))\n",
        "\n",
        "    assert [layer_info(l) for l in layers1] == [layer_info(l) for l in layers2]\n",
        "\n",
        "\n",
        "def assert_compile_parameters_equal(model1: tensorflow.keras.models.Model,\n",
        "                                    model2: tensorflow.keras.models.Model):\n",
        "    def to_dict(obj):\n",
        "        items = dict(__class__=obj.__class__.__name__, **vars(obj))\n",
        "        to_remove = {key for key, value in items.items() if key.endswith(\"_fn\")}\n",
        "        for key in to_remove:\n",
        "            items.pop(key)\n",
        "\n",
        "    assert to_dict(model1.optimizer) == to_dict(model2.optimizer)\n",
        "\n",
        "\n",
        "def loss_name(model):\n",
        "    if isinstance(model.loss, str):\n",
        "        loss = getattr(tensorflow.keras.losses, model.loss)\n",
        "    else:\n",
        "        loss = model.loss\n",
        "    return loss.__name__.lower()\n",
        "\n",
        "\n",
        "def hidden_activations(model):\n",
        "    return [layer.activation\n",
        "            for layer in model.layers[:-1] if hasattr(layer, \"activation\")]\n",
        "\n",
        "\n",
        "def output_activation(model):\n",
        "    return model.layers[-1].activation\n",
        "\n",
        "\n",
        "def root_mean_squared_error(system: np.ndarray, human: np.ndarray):\n",
        "    return ((system - human) ** 2).mean(axis=0) ** 0.5\n",
        "\n",
        "\n",
        "def multi_class_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.argmax(system, axis=1) == np.argmax(human, axis=1))\n",
        "\n",
        "\n",
        "def binary_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.round(system) == human)"
      ],
      "metadata": {
        "id": "PMsdZXk3EMJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02db8a63-aff4-40a9-bb80-eb9bd3130cfd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded successfully.\n",
            "Data saved to 'auto-mpg.hdf5'.\n",
            "Data downloaded successfully.\n",
            "Data saved to: data/uci-har.hdf5\n",
            "Data downloaded successfully.\n",
            "Training data shape: (23968, 104), (23968, 1)\n",
            "Testing data shape: (6194, 104), (6194, 1)\n",
            "Data saved to 'data/income.hdf5'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPV8uEa_EMNt",
        "outputId": "0a115cb4-5da9-4522-93c9-cec6a6180086"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1, typeguard-4.4.2, langsmith-0.3.8\n",
            "collected 3 items                                                                                  \u001b[0m\n",
            "\n",
            "test_nn.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\n",
            "25.1% accuracy for baseline on census income\n",
            "78.6% accuracy for early on census income\n",
            "75.7% accuracy for late on census income\n",
            "\u001b[32m.\u001b[0m\u001b[31m                                                                               [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m________________________________________ test_deep_vs_wide _________________________________________\u001b[0m\n",
            "\n",
            "capsys = <_pytest.capture.CaptureFixture object at 0x7d0ff5c8e810>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_deep_vs_wide\u001b[39;49;00m(capsys):\u001b[90m\u001b[39;49;00m\n",
            "        train_in, train_out, test_in, test_out = load_hdf5(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/auto-mpg.hdf5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       deep, wide = nn.create_auto_mpg_deep_and_wide_networks(\u001b[90m\u001b[39;49;00m\n",
            "            train_in.shape[-\u001b[94m1\u001b[39;49;00m], train_out.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AttributeError: module 'nn' has no attribute 'create_auto_mpg_deep_and_wide_networks'\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_nn.py\u001b[0m:161: AttributeError\n",
            "\u001b[31m\u001b[1m___________________________________________ test_dropout ___________________________________________\u001b[0m\n",
            "\n",
            "capsys = <_pytest.capture.CaptureFixture object at 0x7d0f76714a90>\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_dropout\u001b[39;49;00m(capsys):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        train_in, train_out, test_in, test_out = load_hdf5(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/uci-har.hdf5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# keep only every 10th training example\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        train_out = train_out[::\u001b[94m10\u001b[39;49;00m, :]\u001b[90m\u001b[39;49;00m\n",
            "        train_in = train_in[::\u001b[94m10\u001b[39;49;00m, :]\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       drop, no_drop = nn.create_activity_dropout_and_nodropout_networks(\u001b[90m\u001b[39;49;00m\n",
            "            train_in.shape[-\u001b[94m1\u001b[39;49;00m], train_out.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AttributeError: module 'nn' has no attribute 'create_activity_dropout_and_nodropout_networks'\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_nn.py\u001b[0m:214: AttributeError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_nn.py::\u001b[1mtest_deep_vs_wide\u001b[0m - AttributeError: module 'nn' has no attribute 'create_auto_mpg_deep_and_wide_networks'\n",
            "\u001b[31mFAILED\u001b[0m test_nn.py::\u001b[1mtest_dropout\u001b[0m - AttributeError: module 'nn' has no attribute 'create_activity_dropout_and_nodropout_networks'\n",
            "\u001b[31m============================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 145.95s (0:02:25)\u001b[0m\u001b[31m ==============================\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}