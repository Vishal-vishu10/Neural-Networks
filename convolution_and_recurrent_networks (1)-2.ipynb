{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Ak4If2D-6M",
        "outputId": "65e10f6a-df62-48a3-9ba1-1725d040ab0a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load nn.py\n",
        "\"\"\"nn.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1nncHMaUAaheWr5ATvX32QOENaWY_YMJ0\n",
        "\"\"\"\n",
        "\n",
        "from typing import Tuple, List, Dict\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_toy_rnn(input_shape: tuple, n_outputs: int) -> Tuple[tf.keras.models.Model, Dict]:\n",
        "    \"\"\"Creates a recurrent neural network for a toy problem.\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.SimpleRNN(32, activation='relu', return_sequences=True, input_shape=input_shape),\n",
        "        layers.SimpleRNN(32, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model, {'batch_size': 32, 'epochs': 20}\n",
        "\n",
        "def create_mnist_cnn(input_shape: tuple, n_outputs: int) -> Tuple[tf.keras.models.Model, Dict]:\n",
        "    \"\"\"Creates a convolutional neural network for digit classification.\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model, {'batch_size': 64, 'epochs': 10}\n",
        "\n",
        "def create_youtube_comment_rnn(vocabulary: List[str], n_outputs: int) -> Tuple[tf.keras.models.Model, Dict]:\n",
        "    \"\"\"Creates a recurrent neural network for spam classification.\"\"\"\n",
        "    vocab_size = len(vocabulary)\n",
        "    model = models.Sequential([\n",
        "        layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True),\n",
        "        layers.LSTM(64, return_sequences=True),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model, {'batch_size': 32, 'epochs': 15}\n",
        "\n",
        "def create_youtube_comment_cnn(vocabulary: List[str], n_outputs: int) -> Tuple[tf.keras.models.Model, Dict]:\n",
        "    \"\"\"Creates a convolutional neural network for spam classification.\"\"\"\n",
        "    vocab_size = len(vocabulary)\n",
        "    model = models.Sequential([\n",
        "        layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=100),\n",
        "        layers.Conv1D(64, 5, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(128, 5, activation='relu'),\n",
        "        layers.GlobalMaxPooling1D(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(n_outputs, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model, {'batch_size': 32, 'epochs': 15}\n",
        "\n"
      ],
      "metadata": {
        "id": "8KraeMgbELz0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nn.py', 'r') as file:\n",
        "    code = file.read()"
      ],
      "metadata": {
        "id": "GMDGVG7wEL81"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tUHxeRREMF7",
        "outputId": "da6ee135-e759-412a-cdc8-9ffb0c2c17ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"nn-1.ipynb\n",
            "\n",
            "Automatically generated by Colab.\n",
            "\n",
            "Original file is located at\n",
            "    https://colab.research.google.com/drive/1P7dIWHZ4MR65nH0xIZmt55yxQq9umBam\n",
            "\"\"\"\n",
            "\n",
            "\"\"\"\n",
            "The main code for the recurrent and convolutional networks assignment.\n",
            "See README.md for details.\n",
            "\"\"\"\n",
            "from typing import Tuple, List, Dict\n",
            "\n",
            "import tensorflow\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Input, LSTM, Dense, Conv2D, MaxPool2D, Flatten, Embedding, Conv1D, GlobalMaxPool1D, Dropout, BatchNormalization, TimeDistributed\n",
            "from tensorflow.keras.callbacks import EarlyStopping\n",
            "\n",
            "def create_toy_rnn(input_shape: tuple, n_outputs: int) \\\n",
            "        -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
            "    \"\"\"Creates a recurrent neural network for a toy problem.\"\"\"\n",
            "    model = Sequential([\n",
            "        Input(shape=input_shape),\n",
            "        LSTM(64, return_sequences=True),\n",
            "        BatchNormalization(),\n",
            "        Dropout(0.2),\n",
            "        LSTM(32, return_sequences=True),\n",
            "        BatchNormalization(),\n",
            "        TimeDistributed(Dense(16, activation='relu')),\n",
            "        BatchNormalization(),\n",
            "        Dropout(0.2),\n",
            "        Dense(n_outputs, activation='linear')\n",
            "    ])\n",
            "\n",
            "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=0.015, weight_decay=0.005)\n",
            "    model.compile(optimizer=optimizer,\n",
            "                 loss='mse',\n",
            "                 metrics=['mae'])\n",
            "\n",
            "    fit_kwargs = {\n",
            "        'batch_size': 1,\n",
            "        'callbacks': [EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
            "    }\n",
            "\n",
            "    return model, fit_kwargs\n",
            "\n",
            "def create_mnist_cnn(input_shape: tuple, n_outputs: int) \\\n",
            "        -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
            "    \"\"\"Creates a convolutional neural network for digit classification.\"\"\"\n",
            "    model = Sequential([\n",
            "        Input(shape=input_shape),\n",
            "        Conv2D(32, (3, 3), activation='relu'),\n",
            "        BatchNormalization(),\n",
            "        MaxPool2D((2, 2)),\n",
            "        Conv2D(64, (3, 3), activation='relu'),\n",
            "        BatchNormalization(),\n",
            "        MaxPool2D((2, 2)),\n",
            "        Flatten(),\n",
            "        Dense(128, activation='relu'),\n",
            "        BatchNormalization(),\n",
            "        Dropout(0.5),\n",
            "        Dense(n_outputs, activation='softmax')\n",
            "    ])\n",
            "\n",
            "    model.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001),\n",
            "                 loss='categorical_crossentropy',\n",
            "                 metrics=['accuracy'])\n",
            "\n",
            "    fit_kwargs = {\n",
            "        'batch_size': 32,\n",
            "        'callbacks': [EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
            "    }\n",
            "\n",
            "    return model, fit_kwargs\n",
            "\n",
            "def create_youtube_comment_rnn(vocabulary: List[str], n_outputs: int) \\\n",
            "        -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
            "    \"\"\"Creates a recurrent neural network for spam classification.\"\"\"\n",
            "    vocab_size = len(vocabulary)\n",
            "\n",
            "    model = Sequential([\n",
            "        Input(shape=(None,)),\n",
            "        Embedding(input_dim=vocab_size, output_dim=64, mask_zero=True),  # Restored to 64\n",
            "        LSTM(64, return_sequences=False),  # Restored to 64\n",
            "        BatchNormalization(),\n",
            "        Dropout(0.3),\n",
            "        Dense(32, activation='relu'),  # Restored to 32\n",
            "        BatchNormalization(),\n",
            "        Dropout(0.3),  # Added second dropout\n",
            "        Dense(n_outputs, activation='sigmoid')\n",
            "    ])\n",
            "\n",
            "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=0.001, weight_decay=0.01)\n",
            "    model.compile(optimizer=optimizer,\n",
            "                 loss='binary_crossentropy',\n",
            "                 metrics=['accuracy'])\n",
            "\n",
            "    fit_kwargs = {\n",
            "        'batch_size': 32,\n",
            "        'callbacks': [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]  # Reduced patience\n",
            "    }\n",
            "\n",
            "    return model, fit_kwargs\n",
            "\n",
            "def create_youtube_comment_cnn(vocabulary: List[str], n_outputs: int) \\\n",
            "        -> Tuple[tensorflow.keras.models.Model, Dict]:\n",
            "    \"\"\"Creates a convolutional neural network for spam classification.\"\"\"\n",
            "    vocab_size = len(vocabulary)\n",
            "\n",
            "    model = Sequential([\n",
            "        Input(shape=(None,)),\n",
            "        Embedding(input_dim=vocab_size, output_dim=32),\n",
            "        Conv1D(64, 5, activation='relu'),\n",
            "        GlobalMaxPool1D(),\n",
            "        Dense(32, activation='relu'),\n",
            "        Dense(n_outputs, activation='sigmoid')\n",
            "    ])\n",
            "\n",
            "    model.compile(optimizer='adam',\n",
            "                 loss='binary_crossentropy',\n",
            "                 metrics=['accuracy'])\n",
            "\n",
            "    fit_kwargs = {\n",
            "        'batch_size': 32,\n",
            "        'callbacks': [EarlyStopping(monitor='val_loss', patience=10)]\n",
            "    }\n",
            "\n",
            "    return model, fit_kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load test_nn.py\n",
        "\"\"\"test_nn.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/17ZUKbqFIq3Z_2rGyAuV4yX3z8UaOCSOS\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_input, train_output), (test_input, test_output) = mnist.load_data()\n",
        "\n",
        "# Expand dimensions to make compatible with CNN input shape\n",
        "train_input = np.expand_dims(train_input, axis=-1)\n",
        "test_input = np.expand_dims(test_input, axis=-1)\n",
        "\n",
        "# Convert labels to one-hot encoded vectors\n",
        "train_output = to_categorical(train_output).astype(np.float32)\n",
        "test_output = to_categorical(test_output).astype(np.float32)\n",
        "\n",
        "# Create 'data' directory if it doesn't exist\n",
        "if not os.path.exists('data'):  # For Assignment data is provided in hdf5 format\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Save preprocessed data to HDF5 file\n",
        "with h5py.File('data/mnist.hdf5', 'w') as f:\n",
        "    # Include only every 100th training/testing example to limit dataset size\n",
        "    train = f.create_group(\"train\")\n",
        "    train.create_dataset(\"input\", compression=\"gzip\", data=train_input[::100])\n",
        "    train.create_dataset(\"output\", compression=\"gzip\", data=train_output[::100])\n",
        "    test = f.create_group(\"test\")\n",
        "    test.create_dataset(\"input\", compression=\"gzip\", data=test_input[::100])\n",
        "    test.create_dataset(\"output\", compression=\"gzip\", data=test_output[::100])\n",
        "\n",
        "import os\n",
        "print(os.path.exists(\"/mnt/data/\"))  # Should print True if the directory exists\n",
        "\n",
        "import json\n",
        "import re\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download .csv files from\n",
        "    #     https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection\n",
        "# Correct file paths\n",
        "import pandas as pd\n",
        "\n",
        "# File names\n",
        "names = [\"1-Psy\", \"2-KatyPerry\", \"3-LMFAO\", \"4-Eminem\", \"5-Shakira\"]\n",
        "\n",
        "# Correct file paths\n",
        "file_paths = [\"/content/Youtube0{0}.csv\".format(name) for name in names]\n",
        "\n",
        "# Load datasets\n",
        "dfs = [pd.read_csv(file) for file in file_paths]\n",
        "\n",
        "#print(\"Files loaded successfully!\")\n",
        "\n",
        "tokenize = re.compile(r\"\\d+|[^\\d\\W]+|\\S\").findall\n",
        "dfs_tokenized = [[tokenize(comment) for comment in df[\"CONTENT\"]]\n",
        "                 for df in dfs]\n",
        "\n",
        "index_to_token = [''] + sorted(set(token\n",
        "                                   for comments in dfs_tokenized\n",
        "                                   for tokens in comments\n",
        "                                   for token in tokens))\n",
        "\n",
        "token_to_index = {c: i for i, c in enumerate(index_to_token)}\n",
        "\n",
        "max_tokens = max(len(tokens)\n",
        "                 for comments in dfs_tokenized\n",
        "                 for tokens in comments)\n",
        "\n",
        "with h5py.File('data/youtube-comments.hdf5', 'w') as f:\n",
        "    f.attrs[\"vocabulary\"] = json.dumps(index_to_token)\n",
        "    for name, df, comments in zip(names, dfs, dfs_tokenized):\n",
        "        matrix_in = np.zeros(shape=(len(comments), max_tokens))\n",
        "        for i, tokens in enumerate(comments):\n",
        "            for j, token in enumerate(tokens):\n",
        "                matrix_in[i, j] = token_to_index[token]\n",
        "        matrix_out = df[\"CLASS\"].values.reshape((-1, 1))\n",
        "        group = f.create_group(name)\n",
        "        group.create_dataset(\"input\", compression=\"gzip\", data=matrix_in)\n",
        "        group.create_dataset(\"output\", compression=\"gzip\", data=matrix_out)\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pytest\n",
        "import tensorflow\n",
        "\n",
        "import nn\n",
        "\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def set_seeds():\n",
        "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "    tensorflow.random.set_seed(42)\n",
        "    tensorflow.config.threading.set_intra_op_parallelism_threads(1)\n",
        "    tensorflow.config.threading.set_inter_op_parallelism_threads(1)\n",
        "\n",
        "\n",
        "def test_toy_rnn(capsys):\n",
        "    n_train = 20\n",
        "    n_test = 10\n",
        "    n_timesteps = 20\n",
        "    n_features = 2\n",
        "\n",
        "    # create random input for train and test\n",
        "    train_in = np.random.randint(1, 11, (n_train, n_timesteps, n_features))\n",
        "    test_in = np.random.randint(1, 11, (n_test, n_timesteps, n_features))\n",
        "\n",
        "    # deterministically create output from the random input\n",
        "    def out(matrix_in):\n",
        "        matrix_out = np.zeros(shape=matrix_in.shape[:-1] + (1,))\n",
        "        for i, example in enumerate(matrix_in):\n",
        "            for j, [_, x1] in enumerate(example):\n",
        "                [x0, _] = example[j - 3] if j >= 3 else [0., 0.]\n",
        "                matrix_out[i, j] = x0 - x1\n",
        "        return matrix_out\n",
        "    train_out = out(train_in)\n",
        "    test_out = out(test_in)\n",
        "\n",
        "    # request a model\n",
        "    input_shape = train_in.shape[1:]\n",
        "    (_, _, n_outputs) = train_out.shape\n",
        "    model, kwargs = nn.create_toy_rnn(input_shape, n_outputs)\n",
        "\n",
        "    # check that model contains a recurrent layer\n",
        "    assert any(is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no convolutional layers\n",
        "    assert all(not is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert \"mean\" in loss_name(model)\n",
        "    assert output_activation(model) == tensorflow.keras.activations.linear\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=20, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure error is low enough\n",
        "    rmse = root_mean_squared_error(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1f} RMSE for RNN on toy problem\".format(rmse))\n",
        "    assert rmse < 2\n",
        "\n",
        "\n",
        "def test_image_cnn(capsys):\n",
        "\n",
        "    with h5py.File(\"data/mnist.hdf5\", 'r') as f:\n",
        "        train = f[\"train\"]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        train_in = np.array(train[\"input\"])\n",
        "        test = f[\"test\"]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "        test_in = np.array(test[\"input\"])\n",
        "\n",
        "    # request a model\n",
        "    input_shape = train_in.shape[1:]\n",
        "    (_, n_outputs) = train_out.shape\n",
        "    model, kwargs = nn.create_mnist_cnn(input_shape, n_outputs)\n",
        "\n",
        "    # check that model contains a convolutional layer\n",
        "    assert any(is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no recurrent layers\n",
        "    assert all(not is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert \"categorical\" in loss_name(model)\n",
        "    assert output_activation(model) == tensorflow.keras.activations.softmax\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = multi_class_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for CNN on MNIST sample\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def test_text_rnn(capsys):\n",
        "\n",
        "    with h5py.File(\"data/youtube-comments.hdf5\", 'r') as f:\n",
        "        vocabulary = json.loads(f.attrs[\"vocabulary\"])\n",
        "        train = f[\"1-Psy\"]\n",
        "        train_in = np.array(train[\"input\"])[:, :200]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        test = f[\"5-Shakira\"]\n",
        "        test_in = np.array(test[\"input\"])[:, :200]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "\n",
        "    # request a model\n",
        "    model, kwargs = nn.create_youtube_comment_rnn(vocabulary=vocabulary,\n",
        "                                                  n_outputs=1)\n",
        "\n",
        "    # check that model contains a recurrent layer\n",
        "    assert any(is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no convolutional layers\n",
        "    assert all(not is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert any(x in loss_name(model) for x in [\"hinge\", \"crossentropy\"])\n",
        "    assert output_activation(model) == tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = binary_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for RNN on Youtube comments\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def test_text_cnn(capsys):\n",
        "    # The data below was obtained as in test_text_rnn\n",
        "    with h5py.File(\"data/youtube-comments.hdf5\", 'r') as f:\n",
        "        vocabulary = json.loads(f.attrs[\"vocabulary\"])\n",
        "        train = f[\"1-Psy\"]\n",
        "        train_in = np.array(train[\"input\"])[:, :200]\n",
        "        train_out = np.array(train[\"output\"])\n",
        "        test = f[\"5-Shakira\"]\n",
        "        test_in = np.array(test[\"input\"])[:, :200]\n",
        "        test_out = np.array(test[\"output\"])\n",
        "\n",
        "    # request a model\n",
        "    model, kwargs = nn.create_youtube_comment_cnn(vocabulary=vocabulary,\n",
        "                                                  n_outputs=1)\n",
        "\n",
        "    # check that model contains a convolutional layer\n",
        "    assert any(is_convolution(layer) for layer in layers(model))\n",
        "\n",
        "    # check that model contains no recurrent layers\n",
        "    assert all(not is_recurrent(layer) for layer in layers(model))\n",
        "\n",
        "    # check that output type and loss are appropriate\n",
        "    assert any(x in loss_name(model) for x in [\"hinge\", \"crossentropy\"])\n",
        "    assert output_activation(model) == tensorflow.keras.activations.sigmoid\n",
        "\n",
        "    # set training data, epochs and validation data\n",
        "    kwargs.update(x=train_in, y=train_out,\n",
        "                  epochs=10, validation_data=(test_in, test_out))\n",
        "\n",
        "    # call fit, including any arguments supplied alongside the model\n",
        "    model.fit(**kwargs)\n",
        "\n",
        "    # make sure accuracy is high enough\n",
        "    accuracy = binary_accuracy(model.predict(test_in), test_out)\n",
        "    with capsys.disabled():\n",
        "        print(\"\\n{:.1%} accuracy for CNN on Youtube comments\".format(accuracy))\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "def layers(model: tensorflow.keras.models.Model):\n",
        "    return [x.layer if isinstance(x, tensorflow.keras.layers.Wrapper) else x\n",
        "            for x in model.layers]\n",
        "\n",
        "\n",
        "def is_convolution(layer: tensorflow.keras.layers.Layer):\n",
        "    return layer.__class__.__name__.startswith('Conv')\n",
        "\n",
        "\n",
        "def is_recurrent(layer: tensorflow.keras.layers.Layer):\n",
        "    return isinstance(layer, tensorflow.keras.layers.RNN)\n",
        "\n",
        "\n",
        "def loss_name(model):\n",
        "    if isinstance(model.loss, str):\n",
        "        loss = getattr(tensorflow.keras.losses, model.loss)\n",
        "    else:\n",
        "        loss = model.loss\n",
        "    return loss.__name__.lower()\n",
        "\n",
        "\n",
        "def output_activation(model: tensorflow.keras.models.Model):\n",
        "    return model.layers[-1].activation\n",
        "\n",
        "\n",
        "def root_mean_squared_error(system: np.ndarray, human: np.ndarray):\n",
        "    return ((system - human) ** 2).mean() ** 0.5\n",
        "\n",
        "\n",
        "def multi_class_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.argmax(system, axis=1) == np.argmax(human, axis=1))\n",
        "\n",
        "\n",
        "def binary_accuracy(system: np.ndarray, human: np.ndarray):\n",
        "    return np.mean(np.round(system) == human)\n",
        "\n"
      ],
      "metadata": {
        "id": "PMsdZXk3EMJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b779e7-969f-4034-dab1-93798510c43a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPV8uEa_EMNt",
        "outputId": "13e00cd3-9e3d-4ba2-dbc0-8fab77498944"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1, langsmith-0.3.8, typeguard-4.4.2\n",
            "collected 4 items                                                                                  \u001b[0m\n",
            "\n",
            "test_nn.py \n",
            "1.3 RMSE for RNN on toy problem\n",
            "\u001b[32m.\u001b[0m\n",
            "88.0% accuracy for CNN on MNIST sample\n",
            "\u001b[32m.\u001b[0m\n",
            "86.5% accuracy for RNN on Youtube comments\n",
            "\u001b[32m.\u001b[0m\n",
            "85.7% accuracy for CNN on Youtube comments\n",
            "\u001b[32m.\u001b[0m\u001b[32m                                                                              [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 44.96s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zhe6bNBWmLe7"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}